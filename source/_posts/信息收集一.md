---
title: 信息收集第一步
date: 2020-05-25 15:54:24
tags:
        - bug_bounty
         
categories:
        - bug_bounty
---

信息收集第一步。

<!--more -->

高手们都说“信息收集”决定挖洞的效果，目标多了，自然挖到的可能就多些。有了目标之后，还需要自身技术积累是否强大，这会决定挖洞的质量。

利用网上公开的脚本，自动化来获取想要的信息，这样会节省时间提高效率。

#### 子域名

##### Amass

https://github.com/OWASP/Amass

缺点：速度慢，如果对大量的域名进行子域名枚举的话，会花费几天时间。单个域名枚举的时候，可以使用。

 `amass enum -d baidu.com -o baidu_amass.txt`



#####  crt.sh

https://crt.sh/

HTTPS证书透明度

Google 证书透明度报告项目，解决HTTPS证书系统的结构性缺陷，让所有人查看各个网站的HTTPS证书，从而发现签发了证书的子域名。

 

HTTPS证书内置支持的域名和子域名

https://crt.sh/?q=baidu.com&output=json

`curl -s "https://crt.sh/?q=baidu.com&output=json" | jq -r '.[].name_value'| sed 's/\*\.//g' | sort -u | tee -a baidu.txt`



##### subfinder

https://github.com/projectdiscovery/subfinder

 

Go 语言写的速度快

 

`subfinder -d {0} -o {1}_sub.txt`

##### assetfinder

https://github.com/tomnomnom/assetfinder

Go 编写 

`assetfinder [--subs-only] <domain>`



##### 多个子域名枚举工具+ 子域名托管漏洞验证

利用多个子域名枚举工具，将其结果汇总去重，最后检测是否存在子域名托管漏洞。

```python
import sys
import os

urls = sys.argv[1]

with open(urls, 'r') as f:
    for i in f.readlines():
        url = i.strip('\n')
        print(url)

        a = '''curl -s "https://crt.sh/?q={0}&output=json" | jq -r '.[].name_value'| sed 's/\*\.//g' | sort -u | tee -a {1}_crt.txt'''.format(url,url)
        os.system(a)

        # _amass.txt
        b   = 'amass enum -d {0} -o {1}_amass.txt'.format(url,url)
        os.system(b)

        #_sub.txt
        c = 'subfinder -d {0} -o {1}_sub.txt'.format(url,url)
        os.system(c)
        #asset
        i = 'assetfinder {0} | sort -u > {1}_asset.txt'.format(url,url)
        os.system(i)
        d = 'cat {0}_crt.txt {1}_amass.txt {2}_sub.txt {3}_asset.txt | sort | uniq > {4}.txt'.format(url,url,url,url,url)

        os.system(d)
        #
        e = 'rm -rf {0}_crt.txt {1}_amass.txt {2}_sub.txt {3}_asset.txt'.format(url,url,url,url)
        os.system(e)

       # f = 'cat {0}.txt | aquatone -chrome-path /root/tools/craw/chrome-linux/chrome'.format(url)
        #os.system(f)

        # sub takeover

        g = 'subjack -w {0}.txt  -t 1000 -o takeover_{1}.txt -ssl'.format(url,url)
        os.system(g)
```



##### 域名存活判断

HTTP&HTTPS 判断去重

https://github.com/tomnomnom/httprobe

 

`cat oppo.txt | httprobe > hoppo.txt`

 

获取到的域名可能会有重复的，比如`www.abc.com` http/https都能访问通。

 ```python
import sys

if len(sys.argv) == 1:
    msg = """
    unqiue http(s) domain
    Usage: qcdomain.py input output1(http://www.abc.com) output2(www.abc.com)
        """
    print(msg)
    sys.exit(0)

domains = sys.argv[1]
qdomains = sys.argv[2]
qdomain = sys.argv[3]
b = []
with open(domains,'r') as f:
    for i in f:
        domain = (i.strip('\n').split('//')[1])
        b.append(domain)
        #向列表b中添加域名，然后判断出现一次的时候，直接把 http(s)://www.abc.com输出。即使有重复的，只输出一次
        if b.count(domain) == 1:
            http_domain = i.strip()
            #获取http://www.abc.com 跑敏感目录
            with open(qdomains, 'a+') as d:
                d.writelines(http_domain + '\n')
            #获取 domain   www.abc.com  nmap 跑端口
            with open(qdomain, 'a+') as d:
                d.writelines(http_domain.split('//')[1] + '\n')
 ```



##### **企业组织架构收集信息**

[百度信用](https://xin.baidu.com/)

[天眼查](https://www.tianyancha.com/)

##### 公众号&小程序

[公众号获取](https://github.com/3stoneBrother/personalTools/blob/master/scripts/gongzhonghao.py)

[微信小程序的渗透](https://mp.weixin.qq.com/s/D7_VXLH1Jr3yd_cB8u1MJQ)

#### 敏感信息

#####  BBScan  敏感路径探测

https://github.com/lijiejie/BBScan

 

`[root@vultr BBScan]# python BBScan.py -f /root/tools/domains/src/oppo/qchoop.txt`

 

高危漏洞爆发后，编写规则插件，全网扫描。

##### ffuf 爆破目录 参数爆破

https://github.com/ffuf/ffuf

可以对敏感路径以及参数进行爆破。

批量的扫描。

输出格式不是很好看，可以使用html格式。

`"ffuf -w ../dict/dict.txt -u {0}/FUZZ  -o {1}.html -of html".format(url,domain)`

GET参数

`ffuf -w /path/to/paramnames.txt -u https://target/script.php?FUZZ=test_value -fs 4242`

对域名批量敏感目录爬取。

```python
import os
import sys
domain = sys.argv[1]
with open(domain,'r') as f:
        domains = set(f)
        #num = 51

        for i in domains:
                url = i.strip('\n')
                domain = url.split('//')[1] 
                a = "ffuf -w ../dict/dict.txt -u {0}/FUZZ  -o {1}.html -of html".format(url,domain)
                os.system(a)
```

##### JS文件

[利用js文件进行渗透](https://mp.weixin.qq.com/s?__biz=Mzg4MzA4Nzg4Ng==&mid=2247483903&idx=1&sn=0f8b83fb47477d863ad0fd234c7d9baa&chksm=cf4d8e9ef83a0788f645e5c0142db88f05455460833cf1c64634a0e791df4b8e0d111ea8be32&scene=0&xtrack=1#rd)

[一次敏感信息泄露引发的逻辑漏洞挖掘](https://xz.aliyun.com/t/7745)

工具：

**JSFinder**

https://github.com/Threezh1/JSFinder

https://xz.aliyun.com/t/5390

 

-d 深度爬取，获取更多的内容。-ou, -os来指定URL和子域名所保存的文件名

 `python3 JSFinder.py -u https://www.sec-wiki.com`

` python3 JSFinder.py -u https://www.sec-wiki.com -d -ou url.txt -os domain.txt`

 

缺点：速度慢，不能批量。

**LinkFinder**

https://github.com/GerbenJavado/LinkFinder 

分析整个域及其JS文件：

`python linkfinder.py -i https://example.com -d -o te.html`

**BurpSuite**

https://xz.aliyun.com/t/5409



##### google 搜索脚本

将google常用的几种搜索方法集中到一个脚本里面。

[Fast Google Dorks Scan](https://github.com/IvanGlinkin/Fast-Google-Dorks-Scan/blob/master/FGDS.sh)

`./FGDS.sh app.starbucks.com`

##### 脚本汇总

实现的目标：输入一个域名，对其进行子域名枚举，域名存活判断和敏感目录的扫描，最后结合[xray](https://xray.cool/)的基本爬虫进行扫描。

```python
"""
脚本编写目的：
输入一个子域名，首先去获取子域名，去重。
将子域名进行HTTP(S)判断，由于一个域名http(s)都可以访问。再次去重
获取http://www.abc.com(被动扫描) www.abc.com(nmap)。
敏感路径的扫描 BBscan
爬取url+xray
被动扫描 xray
python3 sub.info.py abc.txt
abc.txt 里面是要爬取的域名：baidu.com
"""
# -*-coding:utf-8 -*-
import sys
import  os

#子域名的获取，输入域名，返回获取到的子域名(3个子域名枚举工具)
def getsubdomain(domain):
    #crt证书
    a = '''curl -s "https://crt.sh/?q={0}&output=json" | jq -r '.[].name_value'| sed 's/\*\.//g' | sort -u | tee -a {1}_crt.txt'''.format(
        domain, domain)
    os.system(a)
    # _amass.txt
    b = 'amass enum -d {0} -o {1}_amass.txt'.format(domain, domain)
    os.system(b)

    # _sub.txt
    c = 'subfinder -d {0} -o {1}_sub.txt'.format(domain, domain)
    os.system(c)
    # # asset
    i = 'assetfinder {0} | sort -u > {1}_asset.txt'.format(domain, domain)
    os.system(i)
    d = 'cat {0}_crt.txt {1}_amass.txt {2}_sub.txt {3}_asset.txt | sort | uniq > {4}.txt'.format(domain, domain, domain, domain,domain)
    #d = 'cat {0}_crt.txt  {1}_asset.txt | sort | uniq > {2}.txt'.format(domain, domain, domain)
    os.system(d)

    #
    e = 'rm -rf {0}_crt.txt {1}_amass.txt {2}_sub.txt {3}_asset.txt'.format(domain, domain, domain, domain)
    os.system(e)

    # f = 'cat {0}.txt | aquatone -chrome-path /root/tools/craw/chrome-linux/chrome'.format(url)
    # os.system(f)

    # sub takeover

    g = 'subjack -w {0}.txt  -t 1000 -o takeover_{1}.txt -ssl'.format(domain, domain)
    os.system(g)

    return "{0}.txt".format(domain)

#对域名的二次处理,subdomain 子域名列表（getsubdomain 函数的输出结果）
def http_s(target,subdomain):
    #subdomain = getsubdomain(target)
    hp = 'cat {0} | httprobe > {1}_http.txt'.format(subdomain,target)
    os.system(hp)
    http = "{}_http.txt".format(target)
    #b 一个空的列表，用来去重的设置
    b = []
    # 获取http://www.abc.com 跑敏感目录
    qchttp = "{}_qchttp.txt".format(target)
    qcsdomain = "{}_qcsdomain.txt".format(target)
    with open(http, 'r') as f:
        for i in f:
            domain = (i.strip('\n').split('//')[1])
            b.append(domain)
            # 向列表b中添加域名，然后判断出现一次的时候，直接把 http(s)://www.abc.com输出。即使有重复的，只输出一次
            if b.count(domain) == 1:
                http_domain = i.strip()

                with open(qchttp, 'a+') as d:
                    d.writelines(http_domain + '\n')
                # 获取 domain   www.abc.com  nmap 跑端口
                with open(qcsdomain, 'a+') as d:
                    d.writelines(http_domain.split('//')[1] + '\n')


    return qchttp

#对获取的域名进行处理，防止扫描到其他未授权的域名 从http_s获取到的结果，和之前输入的域名进行做对比。
#domains 获取到的子域名，dm列表域名
def right_domain(qchttp,domain,dm):
    try:
        qqchttp = "{0}_qqchttp.txt".format(domain)
        with open(dm,'r') as f:
            for i in f:
                dms = i.strip('\n')

                with open(qchttp, 'r') as g:
                    for d in g:
                        domains = d.strip('\n')
                #print(domains)

                        if dms in domains:
                            with open(qqchttp, 'a+') as d:
                                d.writelines(domains + '\n')
        file = "python BBScan.py -f {0}".format(qqchttp)
        os.system(file)
    except:
        pass
    return qqchttp

#扫描的是获取http_s函数输出的结果，作为参数进行调用
def xray(domains):
    #domains = http_s(domain)
    try:
        with open(domains, 'r') as f:
            dms = set(f)
            for i in dms:
                print(i)
                url = i.strip('\n\t')
                dm = url.split('//')[1]

                try:
                    g = "/root/tools/xary/xray_linux_amd64 webscan --basic-crawler {0} --html-output {1}_.html".format(url,dm)
                    os.system(g)
                except:
                    pass
        return ""
    except:
        pass
    
if __name__ == '__main__':
    if len(sys.argv) == 1:
        msg = """
        src_info.py
        Usage: src_info.py target
            """
        print(msg)
        sys.exit(0)
    domains = sys.argv[1]
    with open(domains,'r') as f:
        for i in f:
            domain = i.strip('\n')
            #subdomain = getsubdomain(domain)
            http_domain = right_domain(http_s(domain,getsubdomain(domain)),domain,sys.argv[1])
            #http_s(domain,getsubdomain(domain))
            xray(http_domain)


```

##### 参考

[漏洞悬赏方法论-v4](https://drive.google.com/file/d/1aG_qqRvNW-s5_8vvPk5rJiMSMeNL2uY9/view)
[https://twitter.com/NahamSec](https://twitter.com/NahamSec)








